{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconciling Titles \n",
    "\n",
    "This notebook seeks to reconcile titles in the HathiTrust dataset with the DLL Catalog's work records. Since titles are orders of magnitude more complex than names of authors, I'm using some Natural Language Processing techniques to extract key words from title strings. I then use the Author Reconciliation model's output to narrow the possible candidate matches in the DLL Catalog's work records to works only by the matched author. In other words, if the model matches a particular work's author as \"Virgil\", then only works by Virgil are potential matches for the keys words in the work's title. The words in the title are tokenized and lemmatized so that matching doesn't have to take into account the different case endings in Latin words. Stop words are also removed. The goal is to iterate over the remaining lemmatized tokens and look for matches in the DLL Catalog's filtered work records.\n",
    "\n",
    "Ideally, a title like *Lucretii De Rerum Natura* would be lemmatized to `Lucretius Res Natura` and matched to a similarly lemmatized title in a DLL Catalog work record for Lucretius."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the necessary modules\n",
    "\n",
    "Note that this notebook should be run in a different virtual environment than the one used for the other notebooks in this repository. To use the same virtual environment I used for this notebook, do `conda create --name dllspacy --file requirements-dllspacy.txt` from the root of this repository.\n",
    "\n",
    "You might need to run the following commands separately:\n",
    "\n",
    "```\n",
    "%pip install -U spacy==3.7.5 --no-cache-dir\n",
    "%pip install \"spacy_lookups_data @ git+https://github.com/diyclassics/spacy-lookups-data.git\" --no-cache-dir\n",
    "%pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-3.7.7-py3-none-any.whl\" --no-cache-dir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Necessary Modules\n",
    "\n",
    "This notebook uses the following modules:\n",
    "\n",
    "- `ast`: (Abstract Syntax Tree) to handle processing some items as a string\n",
    "- `collections`: for the Counter, to keep track of confidence scores, etc.\n",
    "- `pandas`: for opening and working with the CSV files\n",
    "- `rapidfuzz`: for probabilistic matching algorithms\n",
    "- `spacy`: for Natural Language Processing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the la_core_web_lg model from LatinCy\n",
    "\n",
    "I'll use a model from [LatinCy](https://huggingface.co/latincy) to remove stop words, tokenize, and lemmatize the titles.\n",
    "\n",
    "On LatinCy, see Patrick J. Burns, “LatinCy: Synthetic Trained Pipelines for Latin NLP,” arXiv: <https://doi.org/10.48550/arXiv.2305.04365>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Latin language model\n",
    "# Note: Ensure you have the 'la_core_web_lg' model installed in your spaCy environment.\n",
    "# You can install it using: %pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-3.7.7-py3-none-any.whl\" --no-cache-dir\n",
    "nlp = spacy.load(\"la_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the STOP_WORDS List from the LatinCy Model\n",
    "\n",
    "The following cell also adds forms of *liber* (\"book\") to the list of stop words, since it is ubiquitous in titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.la import STOP_WORDS\n",
    "\n",
    "# Add forms of liber to the Latin stop words list\n",
    "custom_stop_words = {\"liber\", \"libri\", \"libro\", \"librum\", \"librorum\", \"libris\", \"libros\"}\n",
    "# Combine the default Latin stop words with the custom ones\n",
    "all_stop_words = STOP_WORDS.union(custom_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Pre-Processing the Titles with LatinCy\n",
    "\n",
    "The `extract_primary_title()` function looks for the presence of certain delimiters that separate the primary title from the secondary title. For example, the \"/\" in \"Petri Lombardi Libri IV sententiarum / studio et cura pp. Collegii S. Bonaventurae in lucem editi\" serves this purpose. The function splits such a title and returns just the primary title.\n",
    "\n",
    "The `preprocess()` function:\n",
    "\n",
    "1. Makes all words lower case to avoid accidental differences between, for example, *Omnia* and *omnia*.\n",
    "2. Tokenizes the words in the title if they are composed of latters, not stop words, and not punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten the title, if possible\n",
    "def extract_primary_title(raw_title):\n",
    "    # Split at the first occurrence of any of the delimiters\n",
    "    split_title = re.split(r'[:;/\\\\]', raw_title, maxsplit=1)\n",
    "    return split_title[0].strip()\n",
    "\n",
    "# Preprocessing function using LatinCy\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct and token.is_alpha\n",
    "    ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Comparing Titles to Candidate Matches\n",
    "\n",
    "The following function uses [Jaccard Similarity](https://www.statology.org/jaccard-similarity/) to determine potential matches between two title strings. Briefly, the Jaccard index is a score between 0 and 1 that represents the degree to which two strings are similar. It is based on the formula (number of items in both strings) / (number in either string). It is useful for comparing tokenized strings. Since it doesn't account for differences in word order or misspellings, a fuzzy matching algorithm is applied later in the script to handle those factors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Jaccard similarity\n",
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    return len(set1 & set2) / len(set1 | set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the files\n",
    "\n",
    "- `latin_authors.csv` is one of the outputs of the Greek-Latin Identification model that was deployed in a different notebook (`python/author_matching.ipynb`). It contains only the rows from `data/hathi2.csv` that have been safely categorized as \"Latin\".\n",
    "- `author_inferences.csv` is another output of `python/author_matching.ipynb`. It contains the original author from the `data/hathi2.csv` and the DLL ID of the matched DLL Catalog authority record.\n",
    "- `works_db.csv` contains data from the DLL Catalog's work records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "input_df = pd.read_csv(\"../output/latin_authors.csv\")\n",
    "inferences_df = pd.read_csv(\"../output/author_inferences.csv\")\n",
    "works_df = pd.read_csv(\"../data/works_db.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Preprocessed Titles from works_df\n",
    "\n",
    "This runs the `preprocess()` function described above on the titles in `works_df`. Caching them will mean that the operation can be performed once, instead of every time a new title is processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache preprocessed titles for works_db\n",
    "works_df[\"preprocessed_title\"] = works_df[\"Title\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the Titles and Propose Candidate Matches\n",
    "\n",
    "The following script performs many complex operations that require a detailed description.\n",
    "\n",
    "For each record, the script attempts to associate the given author with a normalized author's identifier using a separate dataframe of author inferences computed in a previous operation (see `python/author_matching.ipynb`).\n",
    "\n",
    "If no match is found among the author inferences, the assumption is that the author is unknown. The record is flagged for manual review.\n",
    "\n",
    "If a match is found among the author inferences, the inferred author_id is retrieved from a nested dictionary structure contained in the distilbert_author column. This ID is then used to filter a third dataset, works_df, to extract the titles of works associated with that particular author.\n",
    "\n",
    "If the filtered list of works is empty—i.e., no known works for the matched author—the record is again flagged for review and skipped.\n",
    "\n",
    "When there are candidate works in the filtered list, the script applies a two-part matching process to compare the title from the input record to each known work title:\n",
    "\n",
    "    1. Jaccard Similarity is computed between tokenized, preprocessed versions of the input title and each candidate work title. This emphasizes lexical overlap.\n",
    "    2. Fuzzy Matching via fuzz.token_sort_ratio is also used to compare the raw strings directly, capturing more flexible matches based on reordering and character similarity.\n",
    "\n",
    "A weighted average score is then calculated for each candidate work title—60% from the Jaccard similarity and 40% from the fuzzy score. The results are sorted by this combined score in descending order.\n",
    "\n",
    "If at least one title scores above a confidence threshold of 0.25, the script appends up to the top three highest-scoring candidates to the output, each with associated metadata: the original author and title, the matched author ID, the matched work title and ID, the similarity score (rounded to 3 decimal places), and a review flag (triggered if the score is below 0.5).\n",
    "\n",
    "If none of the candidates meet the threshold, the script logs the input record without a matched title and flags it for manual review.\n",
    "\n",
    "Finally, the accumulated list of processed rows is written to a new CSV file (`output/candidate_title_matches.csv`), which can then be examined, post-processed, or reviewed for quality assurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an output list\n",
    "output_rows = []\n",
    "\n",
    "# Iterate through each row in input_df\n",
    "for idx, row in input_df.iterrows():\n",
    "    raw_author = row[\"author\"]\n",
    "    raw_title = row[\"title\"]\n",
    "    url = row[\"url\"]\n",
    "\n",
    "    # Match author to distilbert_author (ID)\n",
    "    match = inferences_df[inferences_df[\"author\"] == raw_author]\n",
    "    # If no match is found, flag for review\n",
    "    if match.empty:\n",
    "        output_rows.append({\n",
    "            \"author\": raw_author,\n",
    "            \"title\": raw_title,\n",
    "            \"url\": url,\n",
    "            \"author_id\": None,\n",
    "            \"matched_title\": None,\n",
    "            \"confidence_score\": None,\n",
    "            \"flagged_for_review\": True\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    # If a match is found, extract the distilbert_author data\n",
    "    # Extract distilbert_author data from the dictionary string in inferences_df\n",
    "    distilbert_data = ast.literal_eval(match[\"distilbert_author\"].values[0])\n",
    "    author_id = distilbert_data.get(\"author_id\")\n",
    "\n",
    "    # Filter works_db for this author_id\n",
    "    candidate_works = works_df[works_df[\"DLL Identifier (Author)\"] == author_id].copy()\n",
    "    # If no works are found for this author_id, flag for review\n",
    "    if candidate_works.empty:\n",
    "        output_rows.append({\n",
    "            \"author\": raw_author,\n",
    "            \"title\": raw_title,\n",
    "            \"url\": url,\n",
    "            \"author_id\": author_id,\n",
    "            \"matched_title\": None,\n",
    "            \"matched_work_id\": None,\n",
    "            \"confidence_score\": None,\n",
    "            \"flagged_for_review\": True\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Extract primary title for better matching\n",
    "    short_title = extract_primary_title(raw_title)\n",
    "    # If a match is found, preprocess the input title\n",
    "    input_tokens = preprocess(short_title)\n",
    "\n",
    "    # Score each candidate title\n",
    "    scores = []\n",
    "    for _, work_row in candidate_works.iterrows():\n",
    "        candidate_title = work_row[\"Title\"]\n",
    "        candidate_tokens = work_row[\"preprocessed_title\"]\n",
    "        work_id = work_row[\"DLL Identifier (Work)\"]\n",
    "        # Calculate Jaccard similarity\n",
    "        sim = jaccard_similarity(input_tokens, candidate_tokens)\n",
    "        # Calculate fuzzy matching score\n",
    "        fuzzy_score = fuzz.token_sort_ratio(raw_title, candidate_title) / 100\n",
    "        # Combine scores with weights\n",
    "        combined_score = 0.6 * sim + 0.4 * fuzzy_score  # Weighted combination\n",
    "        scores.append((candidate_title, combined_score, work_id))\n",
    "\n",
    "    # Sort by score\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Prepare output based on scores\n",
    "    # If scores are found and the top score is above 0.25, include them\n",
    "    if scores and scores[0][1] > 0.25:\n",
    "        for top_title, score, work_id in scores[:3]:\n",
    "            output_rows.append({\n",
    "                \"author\": raw_author,\n",
    "                \"title\": raw_title,\n",
    "                \"url\": url,\n",
    "                \"author_id\": author_id,\n",
    "                \"matched_title\": top_title,\n",
    "                \"matched_work_id\": work_id,\n",
    "                \"confidence_score\": round(score, 3),\n",
    "                \"flagged_for_review\": score < 0.5\n",
    "            })\n",
    "    else:\n",
    "        output_rows.append({\n",
    "            \"author\": raw_author,\n",
    "            \"title\": raw_title,\n",
    "            \"url\": url,\n",
    "            \"author_id\": author_id,\n",
    "            \"matched_title\": None,\n",
    "            \"matched_work_id\": None,\n",
    "            \"confidence_score\": None,\n",
    "            \"flagged_for_review\": True\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Output for Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV or examine output\n",
    "output_df = pd.DataFrame(output_rows)\n",
    "output_df.to_csv(\"../output/candidate_title_matches.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dllspacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
